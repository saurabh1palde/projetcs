{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
    "\n",
    "## Features\n",
    "Default : Target variable                                                                                               \n",
    "duration in month : loan duration                                                                                        \n",
    "purpose : purpose of loan                                                                                              \n",
    "credit_amount : loan amount                                                                                            \n",
    "installment as income perc : emi to income percent                                                                      \n",
    "Gender : male or female                                                                                                \n",
    "age : age of the borrower                                                                                              \n",
    "people under maintenance : number of dependents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "os.chdir(\"D:\\Reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv data set\n",
    "data = pd.read_csv('Credit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the top 5 records of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the structure of the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any missing values or not\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the summary of the data\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the count of each category in dependent variable\n",
    "sns.countplot(x='default',data=data)\n",
    "plt.show()\n",
    "print(data['default'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the numeric variables\n",
    "numeric_data = data[['duration_in_month','credit_amount','installment_as_income_perc','present_res_since','age','people_under_maintenance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the categorical variables\n",
    "cat_data = data[['default','purpose','Gender','job']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatters(data, h=None):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\n",
    "    sns.scatterplot(x=\"credit_amount\",y=\"duration_in_month\", hue=h, data=data, ax=ax1)\n",
    "    sns.scatterplot(x=\"age\",y=\"credit_amount\", hue=h, data=data, ax=ax2)\n",
    "    sns.scatterplot(x=\"age\",y=\"duration_in_month\", hue=h, data=data, ax=ax3)\n",
    "    plt.tight_layout()\n",
    "scatters(data,h='Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grant credit per purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_credits = data[\"purpose\"].value_counts().rename(\"Count\").reset_index()\n",
    "n_credits.sort_values(by=[\"Count\"], ascending=False, inplace=True)\n",
    "n_credits.rename(columns={'index':'purpose'}, inplace=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "bar = sns.barplot(x=\"purpose\",y=\"Count\",data=n_credits)\n",
    "bar.set_xticklabels(bar.get_xticklabels(), rotation=60)\n",
    "plt.ylabel(\"Number of granted credits\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spread of credit amount per pupose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes(x,y,r=45):\n",
    "    fig, ax = plt.subplots(figsize=(13,6))\n",
    "    box = sns.boxplot(x=x,y=y, data=data)\n",
    "    box.set_xticklabels(box.get_xticklabels(), rotation=r)\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.tight_layout()\n",
    "boxes(\"purpose\",\"credit_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spread of credit amount per pupose by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes(x,y,h,r=45):\n",
    "    fig, ax = plt.subplots(figsize=(13,6))\n",
    "    box = sns.boxplot(x=x,y=y,hue=h, data=data)\n",
    "    box.set_xticklabels(box.get_xticklabels(), rotation=r)\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.tight_layout()\n",
    "boxes(\"purpose\",\"credit_amount\",'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding (dummy coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purposedf=pd.get_dummies(data['purpose'],drop_first=True)\n",
    "Genderdf=pd.get_dummies(data['Gender'],drop_first=True)\n",
    "jobdf=pd.get_dummies(data['job'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(purposedf, Genderdf, left_index=True, right_index=True)\n",
    "df1 = pd.merge(df, jobdf, left_index=True, right_index=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude outliers\n",
    "#IV = numeric_data.describe()\n",
    "#IV = IV.reset_index(drop=True)\n",
    "#limIV = len(numeric_data)\n",
    "#numeric_data=numeric_data.reset_index(drop=True)\n",
    "#for i in range(0,len(numeric_data.columns)):\n",
    "#    twenty_five=IV.iloc[4][i]\n",
    "#    seventy_five=IV.iloc[6][i]\n",
    "#    iqr=seventy_five-twenty_five\n",
    "#    max_r=seventy_five+1.5*iqr\n",
    "#    min_r=twenty_five-1.5*iqr\n",
    "#    for j in range(0,limIV):\n",
    "#        if numeric_data.iloc[j,i]>max_r:\n",
    "#            numeric_data.iloc[j,i]=max_r\n",
    "#        if numeric_data.iloc[j,i]<min_r:\n",
    "#            numeric_data.iloc[j,i]=min_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X = pd.DataFrame(preprocessing.normalize(numeric_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X.columns = numeric_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge normalized data with the dummy coded data\n",
    "final_df = pd.merge(df1,normalized_X,left_index=True, right_index=True)\n",
    "final_df = pd.merge(final_df,data['default'],left_index=True, right_index=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data in DV and IV and then make splits for training and testing purpose\n",
    "X = final_df.drop('default',axis=1).values\n",
    "Y = final_df['default'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3230d501a664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# check the proportion of 1's and 0's in training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# check the proportion of 1's and 0's in training set\n",
    "sns.countplot(y_train)\n",
    "plt.show()\n",
    "print(pd.Series(y_train).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the proportion of 1's and 0's in test set\n",
    "sns.countplot(y_test)\n",
    "plt.show()\n",
    "print(pd.Series(y_test).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,25)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(x_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(x_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracy)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Accuracy and K values\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup a knn classifier with k neighbors\n",
    "knn_15 = KNeighborsClassifier(n_neighbors=15)\n",
    "knn_15.fit(x_train,y_train)\n",
    "#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\n",
    "knn_15.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred15 = knn_15.predict(x_test)\n",
    "y_pred_train = knn_15.predict(x_train)\n",
    "confusion_matrix(y_test,y_pred15)\n",
    "\n",
    "pd.crosstab(y_test, y_pred15, rownames=['True'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall is very low because of the highly class imbalance\n",
    "# Perform undersampling to exclude the record from majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=final_df[final_df['default']==0]\n",
    "data3=data2.sample(n=400, random_state=1)\n",
    "data3[data3['default']==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the records of category 1 to undersampled data of category 0\n",
    "data4 = data3.append(final_df[final_df['default']==1])\n",
    "data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the proportion of default\n",
    "data4['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data4.drop('default',axis=1)\n",
    "Y1 = data4['default']\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train1,x_test1,y_train1,y_test1 = train_test_split(X1,Y1,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup arrays to store training and test accuracies\n",
    "neighbors1 = np.arange(1,25)\n",
    "train_accuracy1 =np.empty(len(neighbors1))\n",
    "test_accuracy1 = np.empty(len(neighbors1))\n",
    "\n",
    "for i,k in enumerate(neighbors1):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn1.fit(x_train1, y_train1)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy1[i] = knn1.score(x_train1, y_train1)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy1[i] = knn1.score(x_test1, y_test1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracy1)\n",
    "print(test_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors1, test_accuracy1, label='Testing Accuracy')\n",
    "plt.plot(neighbors1, train_accuracy1, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup a knn classifier with k neighbors as 14 as the accuracy is almost same and beyond that point accuracy decreases\n",
    "knn14 = KNeighborsClassifier(n_neighbors=14)\n",
    "knn14.fit(x_train1,y_train1)\n",
    "\n",
    "#15,14,12,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn14.score(x_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred14 = knn14.predict(x_test1)\n",
    "y_pred_train14 = knn14.predict(x_train1)\n",
    "confusion_matrix(y_test1,y_pred14)\n",
    "\n",
    "pd.crosstab(y_test1, y_pred14, rownames=['True'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test1,y_pred14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tunning\n",
    "\n",
    "# Select the best parameters(hyper parameters) for KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold cross validation for k = 16\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=14)\n",
    "scores = cross_val_score(knn_cv,X,Y,cv=10,scoring='accuracy')\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average accuracy as an out of sample accuracy\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again search for the best value of K using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,35)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn_cv_rg = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores_rg = cross_val_score(knn_cv_rg,X,Y,cv=10,scoring='accuracy')\n",
    "    k_scores.append(scores_rg.mean())\n",
    "print(k_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the max value of k_score (accuracy)\n",
    "np.array(k_scores).argmax()\n",
    "k_scores[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of k versus cross validated accuracy\n",
    "plt.plot(k_range,k_scores)\n",
    "plt.xlabel('Range of K')\n",
    "plt.ylabel('Cross Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup a knn classifier with k neighbors\n",
    "knn_11 = KNeighborsClassifier(n_neighbors=11)\n",
    "knn_11.fit(x_train1,y_train1)\n",
    "knn.score(x_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred11 = knn_11.predict(x_test1)\n",
    "y_pred_train11 = knn_11.predict(x_train1)\n",
    "confusion_matrix(y_test1,y_pred11)\n",
    "\n",
    "pd.crosstab(y_test1, y_pred11, rownames=['True'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test1,y_pred11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More efficient parameter tunning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter value that should be searched\n",
    "k_range_gs = range(1,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid : map the parameter names to the values\n",
    "param_grd = dict(n_neighbors = k_range_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gs = KNeighborsClassifier()\n",
    "knn_cv_gs= GridSearchCV(knn_gs,param_grd,cv=10,scoring='accuracy')\n",
    "knn_cv_gs.fit(X1,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the complete results\n",
    "results=pd.DataFrame(knn_cv_gs.cv_results_)\n",
    "results1 = results[['params','mean_test_score','std_test_score','rank_test_score']]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range_gs,results1['mean_test_score'])\n",
    "plt.xlabel('Range of K')\n",
    "plt.ylabel('Cross Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(knn_cv_gs.best_estimator_)\n",
    "#print(knn_cv_gs.best_params_)\n",
    "#print(knn_cv_gs.best_score_)\n",
    "print(knn_cv_gs.best_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup a knn classifier with k neighbors\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_10.fit(x_train1,y_train1)\n",
    "#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\n",
    "knn_10.score(x_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred10 = knn_10.predict(x_test1)\n",
    "y_pred_train10 = knn_10.predict(x_train1)\n",
    "confusion_matrix(y_test1,y_pred10)\n",
    "\n",
    "pd.crosstab(y_test1, y_pred10, rownames=['True'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test1,y_pred10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
